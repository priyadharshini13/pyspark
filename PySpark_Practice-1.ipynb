{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark-Practice.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkuaj6UM0FNly487ATYaCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyadharshini13/pyspark/blob/main/PySpark_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzO2g8H5rXSm",
        "outputId": "40216187-4720-4791-8c37-a0bff7515aba"
      },
      "source": [
        "# Installing pyspark\r\n",
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 49kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=3e534908a7edb183d74fa1d27a93e60f04a6caedba6d99b8eb5b018fd78810c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OihD90_griS3"
      },
      "source": [
        "Feb 15, 2021 - \r\n",
        "\r\n",
        "Practice\r\n",
        "1. Getting spark version\r\n",
        "2. Creating Dataframe\r\n",
        "    \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysy7_CSNr777"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "spark = SparkSession.builder.appName('RDD Example').config('spark some config option', 'some value').getOrCreate()\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Glv0jiHjr1nv",
        "outputId": "61b57fb1-6ab0-45ed-bc77-e57387ea0809"
      },
      "source": [
        "# 1. Getting spark version\r\n",
        "spark.version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.0.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZtTw008rhYN",
        "outputId": "9326163f-6bfe-48d1-8851-0f3c9921e3d2"
      },
      "source": [
        "\r\n",
        "# creating datadrame\r\n",
        "\r\n",
        "# 1. From list \r\n",
        "data = [['Priya','S/D','London'],['Surya','DEVOPS','Scotland']]\r\n",
        "columns = ['NAME','ROLE','LOCATION']\r\n",
        "\r\n",
        "spark.createDataFrame(data,columns).show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------+--------+\n",
            "| NAME|  ROLE|LOCATION|\n",
            "+-----+------+--------+\n",
            "|Priya|   S/D|  London|\n",
            "|Surya|DEVOPS|Scotland|\n",
            "+-----+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2fY0xRWsG38",
        "outputId": "2280a035-cbca-442b-f058-1111aac60a9a"
      },
      "source": [
        "# 2. From Dictionary\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "# Python\r\n",
        "data = {'Name':['Priya','Aarthi'],\r\n",
        "        'Role':['Software Developer','AI SME'],\r\n",
        "        'Location':['London','Edinburgh']}\r\n",
        "\r\n",
        "# data.values() - Get all values in the data dictionary\r\n",
        "# list(data.values()) - Convert these values to list\r\n",
        "# np.array(list(data.values())) - Convert the list to numpy array\r\n",
        "# np.array(list(data.values())).T.tolist() - Transpose the numpy array  and convert it to list\r\n",
        "\r\n",
        "\r\n",
        "# Numpy array is fast and consumes less memory compared to list\r\n",
        "# All elements in the array must be of same type\r\n",
        "\r\n",
        "\r\n",
        "spark.createDataFrame(np.array(list(data.values())).T.tolist(),list(data.keys())).show()\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------+---------+\n",
            "|  Name|              Role| Location|\n",
            "+------+------------------+---------+\n",
            "| Priya|Software Developer|   London|\n",
            "|Aarthi|            AI SME|Edinburgh|\n",
            "+------+------------------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iR0F2ipuZTg",
        "outputId": "2430589b-ae3c-404c-dd4b-8804435527b9"
      },
      "source": [
        "# 3. From tuples\r\n",
        "\r\n",
        "l = [('Alice', 1)]\r\n",
        "\r\n",
        "# Creates dynamic column name - _1, _2 by default is column names are not given\r\n",
        "spark.createDataFrame(l).show()\r\n",
        "\r\n",
        "#creates dataframe with given colum name\r\n",
        "spark.createDataFrame(l, ['Name','Rank']).show()\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+\n",
            "|   _1| _2|\n",
            "+-----+---+\n",
            "|Alice|  1|\n",
            "+-----+---+\n",
            "\n",
            "+-----+----+\n",
            "| Name|Rank|\n",
            "+-----+----+\n",
            "|Alice|   1|\n",
            "+-----+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9jxpXwvMqQ",
        "outputId": "e5537b13-47ca-4d3d-a60b-436ea45a6d5e"
      },
      "source": [
        "# 5. From list of objects\r\n",
        "d = [{'name': 'Alice', 'age': 1}]\r\n",
        "spark.createDataFrame(d).show()\r\n",
        "spark.createDataFrame(d).collect()\r\n",
        "# [Row(age=1, name='Alice')]\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "|age| name|\n",
            "+---+-----+\n",
            "|  1|Alice|\n",
            "+---+-----+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=1, name='Alice')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}
