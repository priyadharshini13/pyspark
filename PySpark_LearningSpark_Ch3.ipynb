{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_1_Oct_12.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHHuuePHzWAnUoXOckRqEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyadharshini13/pyspark/blob/main/PySpark_LearningSpark_Ch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzAlA8dmr5Qw",
        "outputId": "b1a39843-d57a-41ef-cff9-814c42cd344b"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 59.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=c8f3f9eb5005cb622c4dd496be070f1edcd790565ad970474472d6d906e31532\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UxKimq7sXUa"
      },
      "source": [
        "# Import Sparksession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create spark session\n",
        "spark = SparkSession.builder.appName('LearningPySpark').master('local[4]').getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rW04FjpsLQz",
        "outputId": "ceb6c75d-f751-4bb0-fa3c-d5912acb67c2"
      },
      "source": [
        "df = spark.read.text('/content/sample_data/README.md')\n",
        "df.show(truncate=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|This directory includes a few sample datasets to get you started.                                                                                        |\n",
            "|                                                                                                                                                         |\n",
            "|*   `california_housing_data*.csv` is California housing data from the 1990 US                                                                           |\n",
            "|    Census; more information is available at:                                                                                                            |\n",
            "|    https://developers.google.com/machine-learning/crash-course/california-housing-data-description                                                      |\n",
            "|                                                                                                                                                         |\n",
            "|*   `mnist_*.csv` is a small sample of the                                                                                                               |\n",
            "|    [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is                                                                             |\n",
            "|    described at: http://yann.lecun.com/exdb/mnist/                                                                                                      |\n",
            "|                                                                                                                                                         |\n",
            "|*   `anscombe.json` contains a copy of                                                                                                                   |\n",
            "|    [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet); it                                                                         |\n",
            "|    was originally described in                                                                                                                          |\n",
            "|                                                                                                                                                         |\n",
            "|    Anscombe, F. J. (1973). 'Graphs in Statistical Analysis'. American                                                                                   |\n",
            "|    Statistician. 27 (1): 17-21. JSTOR 2682899.                                                                                                          |\n",
            "|                                                                                                                                                         |\n",
            "|    and our copy was prepared by the                                                                                                                     |\n",
            "|    [vega_datasets library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp4CT-ihspLs",
        "outputId": "3611f63b-3a9e-4a96-c035-c12cabbd2e77"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9WXSJw9wvk1",
        "outputId": "89e3004d-a398-4ee6-a470-06486ac3726e"
      },
      "source": [
        "st = df.filter(df.value.contains('.csv'))\n",
        "print(st.count())\n",
        "\n",
        "st1 = df.filter(df.value.contains('*'))\n",
        "print(st1.count())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOBKIgCQQs6Z",
        "outputId": "a9df5cab-2937-4be2-eb35-f1eb80905177"
      },
      "source": [
        "# 1. Defining schemas in dataframe\n",
        "# two ways\n",
        "  # 1. programatic\n",
        "  # 2. DDL(Using Data Definition language)\n",
        "\n",
        "# 1. programatic\n",
        "from pyspark.sql.types import * \n",
        "schema = StructType([StructField('author', StringType(), False), StructField('title', StringType(), False),\n",
        "                     StructField('pages', IntegerType(), False)])\n",
        "\n",
        "# 2. Using DDL\n",
        "schema1 = 'author STRING, title STRING, pages INT'\n",
        "\n",
        "# By default, Spark infers the schema from the data\n",
        "\n",
        "# Define schema for our data using DDL\n",
        "schema_ddl = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING,`Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
        "# Create our static data\n",
        "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
        "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
        "        \"LinkedIn\"]],\n",
        "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
        "        \"twitter\", \"FB\", \"LinkedIn\"]],\n",
        "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
        "        [\"twitter\", \"FB\"]],\n",
        "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
        "        \"twitter\", \"FB\", \"LinkedIn\"]],\n",
        "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
        "        [\"twitter\", \"LinkedIn\"]]\n",
        "        ]\n",
        "\n",
        "blogs = spark.createDataFrame(data, schema_ddl)\n",
        "# blogs.show()\n",
        "# df.show(1, vertical=True)\n",
        "\n",
        "blogs.show(vertical = True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0-------------------------\n",
            " Id        | 1                    \n",
            " First     | Jules                \n",
            " Last      | Damji                \n",
            " Url       | https://tinyurl.1    \n",
            " Published | 1/4/2016             \n",
            " Hits      | 4535                 \n",
            " Campaigns | [twitter, LinkedIn]  \n",
            "-RECORD 1-------------------------\n",
            " Id        | 2                    \n",
            " First     | Brooke               \n",
            " Last      | Wenig                \n",
            " Url       | https://tinyurl.2    \n",
            " Published | 5/5/2018             \n",
            " Hits      | 8908                 \n",
            " Campaigns | [twitter, LinkedIn]  \n",
            "-RECORD 2-------------------------\n",
            " Id        | 3                    \n",
            " First     | Denny                \n",
            " Last      | Lee                  \n",
            " Url       | https://tinyurl.3    \n",
            " Published | 6/7/2019             \n",
            " Hits      | 7659                 \n",
            " Campaigns | [web, twitter, FB... \n",
            "-RECORD 3-------------------------\n",
            " Id        | 4                    \n",
            " First     | Tathagata            \n",
            " Last      | Das                  \n",
            " Url       | https://tinyurl.4    \n",
            " Published | 5/12/2018            \n",
            " Hits      | 10568                \n",
            " Campaigns | [twitter, FB]        \n",
            "-RECORD 4-------------------------\n",
            " Id        | 5                    \n",
            " First     | Matei                \n",
            " Last      | Zaharia              \n",
            " Url       | https://tinyurl.5    \n",
            " Published | 5/14/2014            \n",
            " Hits      | 40578                \n",
            " Campaigns | [web, twitter, FB... \n",
            "-RECORD 5-------------------------\n",
            " Id        | 6                    \n",
            " First     | Reynold              \n",
            " Last      | Xin                  \n",
            " Url       | https://tinyurl.6    \n",
            " Published | 3/2/2015             \n",
            " Hits      | 25568                \n",
            " Campaigns | [twitter, LinkedIn]  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaeaKZ9cZOnb"
      },
      "source": [
        "# https://spark.apache.org/docs/3.2.0/api/python/getting_started/quickstart_df.html\n",
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZX_W6yaZamt",
        "outputId": "b9e67443-cb7c-4409-a4e9-6655e25ef3dc"
      },
      "source": [
        "blogs.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
            "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
            "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
            "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
            "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
            "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
            "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
            "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
            "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
            "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-ZNJ6pwTP5w",
        "outputId": "e10791d0-14ad-423b-de3c-b6f6c4f0f8d6"
      },
      "source": [
        "# check the definition of dataframe\n",
        "blogs.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: integer (nullable = true)\n",
            " |-- First: string (nullable = true)\n",
            " |-- Last: string (nullable = true)\n",
            " |-- Url: string (nullable = true)\n",
            " |-- Published: string (nullable = true)\n",
            " |-- Hits: integer (nullable = true)\n",
            " |-- Campaigns: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQur3Un6ZuXw"
      },
      "source": [
        "# DATAFRAMES\n",
        "Dataframe can be created  \n",
        "1. from pandas\n",
        "2. using RDD tuples- prarllelize\n",
        "3. using rows - Rows(1, ..)\n",
        "4. Defining schema\n",
        "    1. using ddl = 'a STRING, b INT'\n",
        "    2. programatic = StructType(StructField("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu_wOcibZsRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6f4df1-f77a-43fd-f821-952a6d658048"
      },
      "source": [
        "# Data\n",
        "columns = [\"language\",\"users_count\"]\n",
        "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# 1. From RDD\n",
        "# 1.1 from existing RDD\n",
        "  # Without column names\n",
        "df = rdd.toDF()\n",
        "print('Pyspark dataframe from RDD without column names')\n",
        "df.show()\n",
        "df.printSchema()\n",
        "    # _1|    _2|\n",
        "    # Default column name if column name are not specified.\n",
        "print('*****************************************************')\n",
        "df1 = rdd.toDF(columns)\n",
        "print('Pyspark dataframe from RDD with column names')\n",
        "df1.show()\n",
        "df1.printSchema()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pyspark dataframe from RDD without column names\n",
            "+------+------+\n",
            "|    _1|    _2|\n",
            "+------+------+\n",
            "|  Java| 20000|\n",
            "|Python|100000|\n",
            "| Scala|  3000|\n",
            "+------+------+\n",
            "\n",
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: string (nullable = true)\n",
            "\n",
            "*****************************************************\n",
            "Pyspark dataframe from RDD with column names\n",
            "+--------+-----------+\n",
            "|language|users_count|\n",
            "+--------+-----------+\n",
            "|    Java|      20000|\n",
            "|  Python|     100000|\n",
            "|   Scala|       3000|\n",
            "+--------+-----------+\n",
            "\n",
            "root\n",
            " |-- language: string (nullable = true)\n",
            " |-- users_count: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrPABgx3Zr0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2ca642-5ca7-418d-d4d6-30ab87ed78b7"
      },
      "source": [
        "# 1.2  from existing RDD using createDataframe from sparksession\n",
        "df_frmCreateDF = spark.createDataFrame(rdd).toDF(*columns)\n",
        "df_frmCreateDF.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|language|users_count|\n",
            "+--------+-----------+\n",
            "|    Java|      20000|\n",
            "|  Python|     100000|\n",
            "|   Scala|       3000|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Egi1qTO0NS",
        "outputId": "b54e2580-9bd3-45b3-e8ff-41f08110641f"
      },
      "source": [
        "# 2. Using List collection\n",
        "# 2.1 - createDataFrame(list).toDF\n",
        "df_frmList = spark.createDataFrame(data).toDF(*columns)\n",
        "df_frmList.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|language|users_count|\n",
            "+--------+-----------+\n",
            "|    Java|      20000|\n",
            "|  Python|     100000|\n",
            "|   Scala|       3000|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUoloyCRPE_E",
        "outputId": "94eacf5d-38a5-4ff5-962e-0fa95baeaffe"
      },
      "source": [
        "# 2.2 - Using Row Type\n",
        "from pyspark.sql import Row\n",
        "row_data = map(lambda x: Row(*x), data)\n",
        "df_frmList_2 = spark.createDataFrame(row_data, columns)\n",
        "df_frmList_2.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|language|users_count|\n",
            "+--------+-----------+\n",
            "|    Java|      20000|\n",
            "|  Python|     100000|\n",
            "|   Scala|       3000|\n",
            "+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYKNuCTZRHD8",
        "outputId": "e7418946-a1d3-4da3-c566-b507f5f4a549"
      },
      "source": [
        "# 2.3 Using schema\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "\n",
        "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
        "  ]\n",
        "\n",
        "schema = StructType([ \\\n",
        "    StructField(\"firstname\",StringType(),True), \\\n",
        "    StructField(\"middlename\",StringType(),True), \\\n",
        "    StructField(\"lastname\",StringType(),True), \\\n",
        "    StructField(\"id\", StringType(), True), \\\n",
        "    StructField(\"gender\", StringType(), True), \\\n",
        "    StructField(\"salary\", IntegerType(), True) \\\n",
        "  ])\n",
        " \n",
        "df_frmSchema = spark.createDataFrame(data2, schema)\n",
        "df_frmSchema.printSchema()\n",
        "df_frmSchema.show(truncate= False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|id   |gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|James    |          |Smith   |36636|M     |3000  |\n",
            "|Michael  |Rose      |        |40288|M     |4000  |\n",
            "|Robert   |          |Williams|42114|M     |4000  |\n",
            "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
            "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAMnHFK6StDO",
        "outputId": "f977fca0-bfd2-4fa5-c09d-7de025efb897"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/databricks/LearningSparkV2/master/chapter3/data/sf-fire-calls.csv'\n",
        "df = pd.read_csv(url, index_col=0)\n",
        "print(df.head(5))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           UnitID  IncidentNumber  ...          RowID     Delay\n",
            "CallNumber                         ...                         \n",
            "20110016      T13         2003235  ...  020110016-T13  2.950000\n",
            "20110022      M17         2003241  ...  020110022-M17  4.700000\n",
            "20110023      M41         2003242  ...  020110023-M41  2.433333\n",
            "20110032      E11         2003250  ...  020110032-E11  1.500000\n",
            "20110043      B04         2003259  ...  020110043-B04  3.483333\n",
            "\n",
            "[5 rows x 27 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (12,13,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ELVtgdSG4a"
      },
      "source": [
        "# 3. Using datasources\n",
        "# df_csv = spark.read.option('header', True).option('delimiter',',').csv(\"https://raw.githubusercontent.com/databricks/LearningSparkV2/master/chapter3/data/sf-fire-calls.csv\")\n",
        "# df_csv.show()\n",
        "# chapter3/data/sf-fire-calls.csv\n",
        "# https://raw.githubusercontent.com/databricks/LearningSparkV2/master/chapter3/data/sf-fire-calls.csv\n",
        "\n",
        "# dff = spark.read.format('csv').option('header', True).load('https://raw.githubusercontent.com/databricks/LearningSparkV2/master/chapter3/data/sf-fire-calls.csv')\n",
        "# dff.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr8Wp7nMc-Bd"
      },
      "source": [
        "Dec 3, 2021\n",
        "# Columns and Expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXrLlIjic9Sl",
        "outputId": "8c19baa9-6499-4e37-c855-19623a075634"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# 1. Creating column class object using lit()\n",
        "print('1. Creating column class object using lit()')\n",
        "column_obj = lit('column_example')\n",
        "print(column_obj)\n",
        "print('**********************************')\n",
        "# 2. Creating column in a dataframe - mentioning the column names in toDF\n",
        "print('# 2. Creating column in a dataframe - mentioning the column names in toDF---')\n",
        "data=[(\"James\",23),(\"Ann\",40)]\n",
        "df = spark.createDataFrame(data).toDF('names.firstname', 'age')\n",
        "df.printSchema()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Creating column class object using lit()\n",
            "Column<'column_example'>\n",
            "**********************************\n",
            "# 2. Creating column in a dataframe - mentioning the column names in toDF---\n",
            "root\n",
            " |-- names.firstname: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL3Dxr8TenfS"
      },
      "source": [
        "Accessing columns\n",
        "  1. Using dataframe object\n",
        "  2. Using dot operator and backtick\n",
        "  3. Using col()\n",
        "  4. Using col(), backtick"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2_HVzFddZ5E",
        "outputId": "cdfa27a7-db27-4e1a-9756-b6c3329c36e1"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "#   # 1. just passing column name in dataframe object\n",
        "df.select('age').show()\n",
        "\n",
        "# 1. Using df object\n",
        "df.select(df.age).show()\n",
        "df.select(df['age']).show()\n",
        "\n",
        "# 2. Using dot operator and backtick\n",
        "df.select(df['`names.firstname`']).show()\n",
        "\n",
        "# 3. Using col\n",
        "df.select(col('age')).show()\n",
        "\n",
        "# 4. Using col and backtick\n",
        "df.select(col('`names.firstname`')).show()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|age|\n",
            "+---+\n",
            "| 23|\n",
            "| 40|\n",
            "+---+\n",
            "\n",
            "+---+\n",
            "|age|\n",
            "+---+\n",
            "| 23|\n",
            "| 40|\n",
            "+---+\n",
            "\n",
            "+---+\n",
            "|age|\n",
            "+---+\n",
            "| 23|\n",
            "| 40|\n",
            "+---+\n",
            "\n",
            "+---------------+\n",
            "|names.firstname|\n",
            "+---------------+\n",
            "|          James|\n",
            "|            Ann|\n",
            "+---------------+\n",
            "\n",
            "+---+\n",
            "|age|\n",
            "+---+\n",
            "| 23|\n",
            "| 40|\n",
            "+---+\n",
            "\n",
            "+---------------+\n",
            "|names.firstname|\n",
            "+---------------+\n",
            "|          James|\n",
            "|            Ann|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYe5kB5ZhSoo"
      },
      "source": [
        "Arithmetic operation of column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7WStlqzhU9d",
        "outputId": "999c2353-56a5-4cc0-c802-c95fd3086817"
      },
      "source": [
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "\n",
        "#Arthmetic operations\n",
        "df.select((df.col1 + df.col2).alias('Addition(Col1+Col2)')).show()\n",
        "df.select((df.col2 - df.col1).alias('Subtraction(Col2-col1)')).show()\n",
        "# df.select(df.col1 + df.col2).show()\n",
        "# df.select(df.col1 - df.col2).show() \n",
        "# df.select(df.col1 * df.col2).show()\n",
        "# df.select(df.col1 / df.col2).show()\n",
        "# df.select(df.col1 % df.col2).show()\n",
        "\n",
        "# df.select(df.col2 > df.col3).show()\n",
        "# df.select(df.col2 < df.col3).show()\n",
        "# df.select(df.col2 == df.col3).show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|Addition(Col1+Col2)|\n",
            "+-------------------+\n",
            "|                102|\n",
            "|                203|\n",
            "|                304|\n",
            "+-------------------+\n",
            "\n",
            "+----------------------+\n",
            "|Subtraction(Col2-col1)|\n",
            "+----------------------+\n",
            "|                   -98|\n",
            "|                  -197|\n",
            "|                  -296|\n",
            "+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1cW2TKhiJKz"
      },
      "source": [
        "Column functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB6ziFm9iFLh",
        "outputId": "5567d6d7-f66f-4895-8b8b-11a58d151df5"
      },
      "source": [
        "df.sort(df['col2'].desc()).show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "| 300|   4|   4|\n",
            "| 200|   3|   4|\n",
            "| 100|   2|   1|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCveDqz-ODSu"
      },
      "source": [
        "# Row\n",
        "\n",
        "Different ways of creating Row\n",
        "\n",
        "1. Using PySpark.sql - Row\n",
        "    1. With named argument --> Row(id = 1, name=\"Priya\")\n",
        "    2. Without named argument -->  Row(1, 'Priya')\n",
        "2. Create Custom class from Row\n",
        "3. Using Row class on PySpark RDD\n",
        "4. Using Row class - Dataframe\n",
        "    1. named argument\n",
        "    2. unnamed arguments\n",
        "5. Nested row\n",
        "\n",
        "\n",
        "Different ways of accessing rows.\n",
        "1. For accessing rows with unnamed argument - Use index\n",
        "\n",
        "  r1 = Row(1, 'Priya') ==> row[0] #1, row[1] #Priya\n",
        "\n",
        "2. For accessing rows with Named argument \n",
        "    1. Use key  ==>   print(\"r1['id']-->\", r1['id'])\n",
        "    2. Use dot operator  ==> print(r2.country.europe)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn-aoxRM1mWm",
        "outputId": "92bd715f-c224-4b98-c220-1604d428719a"
      },
      "source": [
        "# Different ways of accessing row\n",
        "r1 = [Row(1, 'John'), Row(2, 'Smith')]\n",
        "print(r1)\n",
        "print(r1[0])\n",
        "print(r1[1])\n",
        "print(r1[0][1])\n",
        "print(r1[1][1])\n",
        "print('--------------------Named argument access--------------')\n",
        "r2 = Row(id=1, name='John', country=Row(europe='England',us='Dallas'))\n",
        "print(r2['id'])\n",
        "print(r2['name'])\n",
        "print(r2.country.europe)\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Row(1, 'John')>, <Row(2, 'Smith')>]\n",
            "<Row(1, 'John')>\n",
            "<Row(2, 'Smith')>\n",
            "John\n",
            "Smith\n",
            "--------------------Named argument access--------------\n",
            "1\n",
            "John\n",
            "England\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UCVuCh_OHVp",
        "outputId": "5710b0f9-535b-4e46-8043-543cd3b46854"
      },
      "source": [
        "# 1. Using pyspark.sql - Row\n",
        "from pyspark.sql import Row\n",
        "\n",
        "#without named argument\n",
        "blog_row = Row(1, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",[\"twitter\", \"LinkedIn\"])\n",
        "# access row using index\n",
        "print(blog_row[1])\n",
        "print(blog_row[-1])\n",
        "print(blog_row[-1][-1])\n",
        "\n",
        "# With named argument\n",
        "blog1_row = Row(id=1, firstname=\"Reynold\")\n",
        "print('--------------------------------------')\n",
        "print('Named argument row:')\n",
        "print('blog1_row----', blog1_row)\n",
        "print(\"blog1_row['firstname']----\",blog1_row['firstname'])\n",
        "print('blog1_row.id-----',blog1_row.id)\n",
        "\n",
        "\n",
        "print('without named argument - ', blog_row)\n",
        "print('With named argument - ', blog1_row )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reynold\n",
            "['twitter', 'LinkedIn']\n",
            "LinkedIn\n",
            "--------------------------------------\n",
            "Named argument row:\n",
            "blog1_row---- Row(id=1, firstname='Reynold')\n",
            "blog1_row['firstname']---- Reynold\n",
            "blog1_row.id----- 1\n",
            "without named argument -  <Row(1, 'Reynold', 'Xin', 'https://tinyurl.6', 255568, '3/2/2015', ['twitter', 'LinkedIn'])>\n",
            "With named argument -  Row(id=1, firstname='Reynold')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0y_TPuiSYKD",
        "outputId": "737b9bea-bf91-439b-f91c-1c58e3e70475"
      },
      "source": [
        "# 2. Create Custom class from Row\n",
        "Person = Row(\"Name\", \"Age\")\n",
        "p1 = Person(\"James\", 40)\n",
        "p2 = Person(\"Robert\", 50)\n",
        "print(p1)\n",
        "print(p2.Name)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Name='James', Age=40)\n",
            "Robert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxxkgcldTsPC",
        "outputId": "9808de29-f023-44fb-d9af-a5e3f0821ca4"
      },
      "source": [
        "# 3. Using Row class on pyspark RDD\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Method 1\n",
        "data = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "print(rdd.collect())\n",
        "for item in rdd.collect():\n",
        "  print(item[0])\n",
        "\n",
        "# Method 2 - using custom class\n",
        "Person = Row('name', 'country')\n",
        "data = [Person(\"Matei Zaharia\", \"CA\"), Person(\"Reynold Xin\", \"CA\")]\n",
        "print(data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Row('Matei Zaharia', 'CA')>, <Row('Reynold Xin', 'CA')>]\n",
            "Matei Zaharia\n",
            "Reynold Xin\n",
            "[Row(name='Matei Zaharia', country='CA'), Row(name='Reynold Xin', country='CA')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXqYjaA8WM0Y",
        "outputId": "0e333b0f-bd94-4e44-b013-3bf3a656950c"
      },
      "source": [
        "# 4. Using Row class on pyspark dataframe\n",
        "\n",
        "from pyspark.sql import Row\n",
        "# Unnamed argument\n",
        "data = [Row(1, 'Priya'), Row(2, 'Dharshini')]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# Named argument\n",
        "data1 = [Row(id=1, name='Priya'), Row(id=2, name='Dharshini')]\n",
        "df1 = spark.createDataFrame(data1)\n",
        "\n",
        "\n",
        "  # using unnamed argument but explicit column name to create dataframe\n",
        "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
        "\n",
        "# Using toDF\n",
        "# columns = ['Authors', 'Country']\n",
        "# df2=spark.createDataFrame(data).toDF(*columns)\n",
        "df2 = spark.createDataFrame(rows,['Authors', 'Country'])\n",
        "\n",
        "print('Un-named argument')\n",
        "df.show()\n",
        "print('Named argument')\n",
        "df1.show()\n",
        "print('explicit column name')\n",
        "df2.show()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Un-named argument\n",
            "+---+---------+\n",
            "| _1|       _2|\n",
            "+---+---------+\n",
            "|  1|    Priya|\n",
            "|  2|Dharshini|\n",
            "+---+---------+\n",
            "\n",
            "Named argument\n",
            "+---+---------+\n",
            "| id|     name|\n",
            "+---+---------+\n",
            "|  1|    Priya|\n",
            "|  2|Dharshini|\n",
            "+---+---------+\n",
            "\n",
            "explicit column name\n",
            "+-------------+-------+\n",
            "|      Authors|Country|\n",
            "+-------------+-------+\n",
            "|Matei Zaharia|     CA|\n",
            "|  Reynold Xin|     CA|\n",
            "+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3NxZh9qWbPm",
        "outputId": "756bed44-b0e1-4265-f31b-f74d6ee2761d"
      },
      "source": [
        "# 5. Nested struct using rows\n",
        "data = [Row(id=1, props=Row(firstname='Priya', lastname='Dharshini'))]\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "# print(df.first())\n",
        "for item in df.collect():\n",
        "  print(\"item['id']-->\", item['id'])\n",
        "  print(\"item['props']-->\", item['props'])\n",
        "  print(\"item['props']['firstname']-->\",item['props']['firstname'])\n",
        "  print(type(item))\n",
        "  print(\"item.props-->\",item.props)\n",
        "  print('item.props.lastname-->', item.props.lastname)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "| id|             props|\n",
            "+---+------------------+\n",
            "|  1|{Priya, Dharshini}|\n",
            "+---+------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- props: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            "\n",
            "item['id']--> 1\n",
            "item['props']--> Row(firstname='Priya', lastname='Dharshini')\n",
            "item['props']['firstname']--> Priya\n",
            "<class 'pyspark.sql.types.Row'>\n",
            "item.props--> Row(firstname='Priya', lastname='Dharshini')\n",
            "item.props.lastname--> Dharshini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJJNy889OmEy",
        "outputId": "c4c2560a-0af4-4575-8906-aef5a85444cf"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------+\n",
            "|      Authors|Country|\n",
            "+-------------+-------+\n",
            "|Matei Zaharia|     CA|\n",
            "|  Reynold Xin|     CA|\n",
            "+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}